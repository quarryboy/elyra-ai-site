{"componentChunkName":"component---src-pages-user-guide-runtime-configuration-mdx","path":"/user-guide/runtime-configuration/","result":{"pageContext":{"frontmatter":{"title":"Runtime configuration","description":"Runtime configuration"},"relativePagePath":"/user-guide/runtime-configuration.mdx","titleType":"page","MdxNode":{"id":"798d0357-bb88-5051-9ac6-1a739df73468","children":[],"parent":"bfd601a5-4f23-5212-b323-212bed30bb76","internal":{"content":"---\ntitle: Runtime configuration\ndescription: Runtime configuration\n---\n\nexport const Title = () => (\n  <span>\n    Runtime configuration\n  </span>\n);\n\n<PageDescription>\n\nA runtime configuration provides Elyra access to external resources, such as Kubeflow Pipelines or Apache Airflow for scalable pipeline execution.\n\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Prerequisites</AnchorLink>\n  <AnchorLink>Managing runtime configurations using the JupyterLab UI</AnchorLink>\n  <AnchorLink>Managing runtime configurations using the Elyra CLI</AnchorLink>\n  <AnchorLink>Configuration settings</AnchorLink>\n  <AnchorLink>Verifying runtime configurations</AnchorLink>\n  <AnchorLink>Troubleshooting</AnchorLink>\n</AnchorLinks>\n\nYou can manage runtime configurations using the [JupyterLab UI](#managing-runtime-configurations-using-the-jupyterlab-ui) or the [Elyra CLI](#managing-runtime-configurations-using-the-elyra-cli).\n\n## Prerequisites\n\nA runtime configuration requires connectivity details for \n* A Kubeflow Pipelines deployment or an Apache Airflow deployment\n* S3-based Object Storage (e.g. Minio or IBM Cloud Object Storage)\n\nNote: Elyra is only tested with Kubeflow v1.2.x and v1.3.x and Apache Airflow v1.10.x.\n\n## Managing runtime configurations using the JupyterLab UI\n\nTo create, edit, or delete runtime configurations using the UI select the `Runtimes` tab from the JupyterLab sidebar, or click the `Runtimes` button in the Pipeline Editor.\n\n  ![Access runtime configurations](../images/ai/access-runtime-configurations.png)\n\n### Creating a runtime configuration\n\nTo create a runtime configuration:\n1. Select the `Runtimes` tab from the JupyterLab sidebar.\n1. Click `+` to add a new runtime configuration and choose the desired runtime configuration type, e.g. Kubeflow Pipelines or Apache Airflow. \n   ![Create runtime configuration](../images/ai/runtime-create-config.png)\n1. Provide a runtime configuration display name, an optional description, and tag the configuration to make it more easily discoverable. \n1. Enter the Kubeflow Pipelines or Apache Airflow deployment information. Refer to section [Kubeflow Pipelines configuration settings](#kubeflow-pipelines-configuration-settings) or [Apache Airflow configuration settings](#apache-airflow-configuration-settings) for details.\n1. Enter the Cloud Storage connectivity information. Refer to section [Cloud Storage settings](#cloud-storage-settings) for details.\n1. Save the runtime configuration. The new entry is displayed in the list.\n1. Expand the entry and verify that you can access the Kubeflow Pipelines or Apache Airflow GUI and the Cloud Storage GUI using the displayed links.\n   ![Access runtime configuration](../images/ai/runtime-access-config.png) \n\n### Modifying a runtime configuration\n\nTo edit a runtime configuration:\n1. Select the `Runtimes` tab from the JupyterLab sidebar.\n1. Click the pencil next to the runtime configuration.\n\n### Deleting a runtime configuration\n\nTo delete a runtime configuration:\n1. Select the `Runtimes` tab from the JupyterLab sidebar.\n1. Click the trash can next to the runtime configuration.\n\n## Managing runtime configurations using the Elyra CLI\n\nYou can list, create, edit, or delete runtime configurations using the `elyra-metadata` CLI.\n\n### Listing runtime configurations\n\nTo list runtime configurations run\n\n```\nelyra-metadata list runtimes\n```\n\nThe output lists for each runtime the name and the name of the associated JSON formatted metadata file, which is stored in the JupyterLab data directory in the `metadata/runtimes` subdirectory.\n\n```\nAvailable metadata instances for runtimes (includes invalid):\n\nSchema   Instance  Resource  \n------   --------  -------- \nkfp      my_kfp    /Users/jdoe/Library/Jupyter/metadata/runtimes/my_kfp.json\n```\n\nTo format the output as JSON run `elyra-metadata list runtimes --json`. Note that the JSON export includes the content of the metadata files, not just their names.\n\n### Creating a runtime configuration\n\nTo create a runtime configuration for a Kubeflow Pipelines deployment:\n\n```bash\nelyra-metadata install runtimes \\\n       --display_name=\"My Kubeflow Pipelines Runtime\" \\\n       --api_endpoint=https://kubernetes-service.ibm.com/pipeline \\\n       --api_username=username@email.com \\\n       --api_password=mypassword \\\n       --engine=Argo \\\n       --cos_endpoint=http://minio-service.kubeflow:9000 \\\n       --cos_username=minio \\\n       --cos_password=minio123 \\\n       --cos_bucket=test-bucket \\\n       --tags=\"['kfp', 'v1.0']\" \\\n       --schema_name=kfp\n```\n\nRefer to the [Kubeflow Pipelines Configuration settings](#kubeflow-pipelines-configuration-settings) section for an explanation of the parameters.\n\n### Modifying a runtime configuration\n\nTo edit a runtime configuration:\n\n```bash\nelyra-metadata install runtimes \\\n       --replace \\\n       --name=\"my_kubeflow_pipelines_runtime\" \\\n       --display_name=\"My Kubeflow Pipelines Runtime\" \\\n       --api_endpoint=https://kubernetes-service.ibm.com/pipeline \\\n       --api_username=username@email.com \\\n       --api_password=mynewpassword \\\n       --engine=Argo \\\n       --cos_endpoint=http://minio-service.kubeflow:9000 \\\n       --cos_username=minio \\\n       --cos_password=minio123 \\\n       --cos_bucket=test-bucket \\\n       --tags=\"['kfp', 'v1.1']\" \\\n       --schema_name=kfp\n```\n\nRefer to the [Kubeflow Pipelines Configuration settings](#kubeflow-pipelines-configuration-settings) section for an explanation of the parameters. Note that you must specify the `--name` parameter. \n\n### Deleting a runtime configuration\n\nTo delete a runtime configuration run the following command, replacing the configuration name as appropriate.\n\n```bash\nelyra-metadata remove runtimes --name=my_kubeflow_pipelines_runtime\n```\n\n## Configuration settings\n\n### Common configuration settings\n\nConfigurations include the following   common settings for all supported runtime types. The string in the headings below, which is enclosed in parentheses, denotes the CLI option name.\n\n#### Name (display_name)\n\nA user-friendly name for runtime configuration. This property is required.\n\nExample: `Kubeflow Pipelines dev environment`\n\n#### N/A (name)\n\nA unique identifier for this configuration. A value is automatically generated from `display_name`.\n\nExample: `kubeflow_pipelines_dev_environment`\n\n#### Description (description)\n\nDescription for this runtime image configuration. This property is optional.\n\nExample: `Kubeflow Pipelines deployment in QA`\n\n#### Tags (tags)\n\nZero or more tags for this runtime configuration.\n\nExample: `['test-env','airflow']`\n\n### Kubeflow Pipelines configuration settings\n\nThis section defines the settings for the Kubeflow Pipelines deployment that you want to associate with this runtime configuration.\n\n#### Kubeflow Pipelines API endpoint (api_endpoint)\n\nThe KubeFlow Pipelines API endpoint you want to utilize. This setting is required.\n\nExample: `https://kubernetes-service.ibm.com/pipeline`\n\n#### Kubeflow Pipelines user namespace (user_namespace)\nThe namespace used to run your pipeline in Kubeflow Pipelines. This setting is required if namespaces are defined in Kubeflow Pipelines. SEE NOTE.\n\nExample: `anonymous`\n\n#### Kubeflow Pipelines API endpoint username (api_username)\nUsername used to access your KubeFlow Pipelines API endpoint. This setting is required if the Kubeflow Pipelines deployment is multi-user, auth enabled.\n\nExample: `username@email.com`\n\n#### Kubeflow Pipelines API endpoint (api_password)\nPassword used to access your KubeFlow Pipelines API endpoint. This setting is required if the Kubeflow Pipelines deployment is multi-user, auth enabled.\n\nExample: `mypassword`\n\n#### Kubeflow Pipelines engine (engine)\nThe engine being used by Kubeflow Pipelines to run pipelines: `Argo` or `Tekton`. If you have access to the Kubernetes cluster where Kubeflow Pipelines is deployed, run these commands in a terminal window to determine the engine type.\n\n```\n# If this command completes successfully, the engine type is Argo.\nkubectl describe configmap -n kubeflow workflow-controller-configmap\n\n# If this command completes successfully, the engine type is Tekton.\nkubectl describe configmap -n kubeflow kfp-tekton-config\n```\n\nThe default is `Argo`.\n\nExample: `Argo`\n\n### Apache Airflow configuration settings\n\nThis section defines the settings for the Apache Airflow deployment that you want to associate with this runtime configuration.\n\n#### Apache Airflow UI endpoint (api_endpoint)\n\nThe Apache Airflow API endpoint you want to utilize. This setting is required.\n\nExample: `https://your-airflow-webserver:port`\n\n#### Apache Airflow user namespace (user_namespace)\nThe namespace used to run your DAG in Apache Airflow. The Kubernetes namespace must be configured with the correct permissions prior to use in Apache Airflow. This setting is Optional. \n\nThe default namespace is `default`.\n\nExample: `anonymous`\n\n#### GitHub API Endpoint (github_api_endpoint)\n\nThe GitHub (or GitHub Enterprise) API endpoint where the git client will attempt to connect. This setting is required. Keep the default  `https://api.github.com` for github.com\n\nExample: `https://api.private.githubenterprise.com`\n\n#### GitHub DAG Repository (github_repo)\n\nThe GitHub repository that Apache Airflow utilizes to store DAGs. This setting is required and the repository must exist.\n\nExample: `user-or-org/dag-repo-name`\n\n#### GitHub DAG Repository Branch (github_branch)\nThe name of the branch in `github_repo` where DAGs are stored. \nThis setting is required and the branch must exist.\n\nExample: `dag-branch`\n\n#### GitHub Personal Access Token (github_repo_token)\nA [GitHub personal access token](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) with write access to the GitHub DAG Repository. This setting is required. \n\nExample: `766f7c267519fee7c71d7f96bdf42e646dc65433`\n\n### Cloud Storage settings\n\nThis section defines the settings for the cloud storage that you want to associate with this runtime configuration.\n\n#### Cloud Object Storage endpoint (cos_endpoint)\nThis should be the URL address of your S3-compatible Object Storage. If running an Object Storage Service within a Kubernetes cluster (Minio), you can use the Kubernetes local DNS address. This setting is required.\n\nExample: `https://minio-service.kubeflow:9000`\n\n#### Cloud Object Storage Credentials Secret (cos_secret)\n(Optional) Kubernetes secret that's defined in the specified user namespace, containing the Cloud Object Storage username and password.\nIf specified, this secret must exist on the Kubernetes cluster hosting your pipeline runtime in order to successfully\nexecute pipelines. This setting is optional but is recommended for use in shared environments to avoid exposing a user's \nCloud Object Storage credentials. \n\nExample: `my-cos-secret`\n\nThe following is an example of how your secret on the Kubernetes cluster hosting your runtime should be defined. The variable\nnames defined under `data`, must be `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` followed by each respective value \nencoded in base64. Learn how to create, deploy, or configure [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/).\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <cos_secret>\ntype: Opaque\ndata:\n  AWS_ACCESS_KEY_ID: <BASE64_ENCODED_YOUR_AWS_ACCESS_KEY_ID>\n  AWS_SECRET_ACCESS_KEY: <BASE64_ENCODED_YOUR_AWS_SECRET_ACCESS_KEY>\n```\n\n#### Cloud Object Storage username (cos_username)\nUsername used to access the Object Storage. This setting is required.\n\nExample: `minio`\n\n#### Cloud Object Storage password (cos_password)\nPassword for cos_username. This setting is required.\n\nExample: `minio123`\n\n#### Cloud Object Storage bucket name (cos_bucket)\nName of the bucket you want Elyra to store pipeline artifacts in. This setting is required. If the bucket doesn't exist, it will be created. The specified bucket name must meet the naming conventions imposed by the Object Storage service.\n\nExample: `test-bucket`\n\n> If using IBM Cloud Object Storage, you must generate a set of [HMAC Credentials](https://cloud.ibm.com/docs/services/cloud-object-storage/hmac?topic=cloud-object-storage-uhc-hmac-credentials-main)\nand grant that key at least [Writer](https://cloud.ibm.com/docs/services/cloud-object-storage/iam?topic=cloud-object-storage-iam-bucket-permissions) level privileges.\nSpecify `access_key_id` and `secret_access_key` as `cos_username` and `cos_password`, respectively.\n\n## Verifying runtime configurations\n\nThe [Elyra examples repository contains a basic pipeline](https://github.com/elyra-ai/examples/pipelines/setup_validation) that you can use to verify your runtime configurations:\n\n1. Launch JupyterLab.\n1. Clone `https://github.com/elyra-ai/examples.git` (`Git` > `Clone A Repository`) into the current working directory.\n1. In the File Browser navigate to `examples/pipelines/setup_validation/` and follow the instructions in `README.md`. If your runtime configuration is correct and the target runtime environment configured correctly, the validation pipeline should run as is without any modifications.\n\n## Troubleshooting \n\nI am seeing this error when using Elyra with Kubeflow Pipelines that is Dex enabled: \n```bash\nHTTP response body: {\"error\":\"Validate experiment request failed.: Invalid input error: Invalid resource references for experiment. Expect one namespace type with owner relationship.\n```\n- Ensure that you have logged into the Kubeflow Dex landing page (https://kubeflow.cluster:31380....) at least once with \nyour credentials via the GUI. You should have been greeted with a dialog box and request to create a new namespace. \nWithout this step complete, Elyra will not be able to create pipelines on the Kubeflow cluster. \n\n- Ensure you've configured Kubeflow Pipelines credentials and that they are correct. When using Dex, the `api_username` is typically \nyour email address and `user_namespace` is your email shortname (e.g. `elyra` for `elyra@email.org`).\n","type":"Mdx","contentDigest":"702cf3b76f3f2c43b5fc4a76c5d88ff2","owner":"gatsby-plugin-mdx","counter":335},"frontmatter":{"title":"Runtime configuration","description":"Runtime configuration"},"exports":{},"rawBody":"---\ntitle: Runtime configuration\ndescription: Runtime configuration\n---\n\nexport const Title = () => (\n  <span>\n    Runtime configuration\n  </span>\n);\n\n<PageDescription>\n\nA runtime configuration provides Elyra access to external resources, such as Kubeflow Pipelines or Apache Airflow for scalable pipeline execution.\n\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Prerequisites</AnchorLink>\n  <AnchorLink>Managing runtime configurations using the JupyterLab UI</AnchorLink>\n  <AnchorLink>Managing runtime configurations using the Elyra CLI</AnchorLink>\n  <AnchorLink>Configuration settings</AnchorLink>\n  <AnchorLink>Verifying runtime configurations</AnchorLink>\n  <AnchorLink>Troubleshooting</AnchorLink>\n</AnchorLinks>\n\nYou can manage runtime configurations using the [JupyterLab UI](#managing-runtime-configurations-using-the-jupyterlab-ui) or the [Elyra CLI](#managing-runtime-configurations-using-the-elyra-cli).\n\n## Prerequisites\n\nA runtime configuration requires connectivity details for \n* A Kubeflow Pipelines deployment or an Apache Airflow deployment\n* S3-based Object Storage (e.g. Minio or IBM Cloud Object Storage)\n\nNote: Elyra is only tested with Kubeflow v1.2.x and v1.3.x and Apache Airflow v1.10.x.\n\n## Managing runtime configurations using the JupyterLab UI\n\nTo create, edit, or delete runtime configurations using the UI select the `Runtimes` tab from the JupyterLab sidebar, or click the `Runtimes` button in the Pipeline Editor.\n\n  ![Access runtime configurations](../images/ai/access-runtime-configurations.png)\n\n### Creating a runtime configuration\n\nTo create a runtime configuration:\n1. Select the `Runtimes` tab from the JupyterLab sidebar.\n1. Click `+` to add a new runtime configuration and choose the desired runtime configuration type, e.g. Kubeflow Pipelines or Apache Airflow. \n   ![Create runtime configuration](../images/ai/runtime-create-config.png)\n1. Provide a runtime configuration display name, an optional description, and tag the configuration to make it more easily discoverable. \n1. Enter the Kubeflow Pipelines or Apache Airflow deployment information. Refer to section [Kubeflow Pipelines configuration settings](#kubeflow-pipelines-configuration-settings) or [Apache Airflow configuration settings](#apache-airflow-configuration-settings) for details.\n1. Enter the Cloud Storage connectivity information. Refer to section [Cloud Storage settings](#cloud-storage-settings) for details.\n1. Save the runtime configuration. The new entry is displayed in the list.\n1. Expand the entry and verify that you can access the Kubeflow Pipelines or Apache Airflow GUI and the Cloud Storage GUI using the displayed links.\n   ![Access runtime configuration](../images/ai/runtime-access-config.png) \n\n### Modifying a runtime configuration\n\nTo edit a runtime configuration:\n1. Select the `Runtimes` tab from the JupyterLab sidebar.\n1. Click the pencil next to the runtime configuration.\n\n### Deleting a runtime configuration\n\nTo delete a runtime configuration:\n1. Select the `Runtimes` tab from the JupyterLab sidebar.\n1. Click the trash can next to the runtime configuration.\n\n## Managing runtime configurations using the Elyra CLI\n\nYou can list, create, edit, or delete runtime configurations using the `elyra-metadata` CLI.\n\n### Listing runtime configurations\n\nTo list runtime configurations run\n\n```\nelyra-metadata list runtimes\n```\n\nThe output lists for each runtime the name and the name of the associated JSON formatted metadata file, which is stored in the JupyterLab data directory in the `metadata/runtimes` subdirectory.\n\n```\nAvailable metadata instances for runtimes (includes invalid):\n\nSchema   Instance  Resource  \n------   --------  -------- \nkfp      my_kfp    /Users/jdoe/Library/Jupyter/metadata/runtimes/my_kfp.json\n```\n\nTo format the output as JSON run `elyra-metadata list runtimes --json`. Note that the JSON export includes the content of the metadata files, not just their names.\n\n### Creating a runtime configuration\n\nTo create a runtime configuration for a Kubeflow Pipelines deployment:\n\n```bash\nelyra-metadata install runtimes \\\n       --display_name=\"My Kubeflow Pipelines Runtime\" \\\n       --api_endpoint=https://kubernetes-service.ibm.com/pipeline \\\n       --api_username=username@email.com \\\n       --api_password=mypassword \\\n       --engine=Argo \\\n       --cos_endpoint=http://minio-service.kubeflow:9000 \\\n       --cos_username=minio \\\n       --cos_password=minio123 \\\n       --cos_bucket=test-bucket \\\n       --tags=\"['kfp', 'v1.0']\" \\\n       --schema_name=kfp\n```\n\nRefer to the [Kubeflow Pipelines Configuration settings](#kubeflow-pipelines-configuration-settings) section for an explanation of the parameters.\n\n### Modifying a runtime configuration\n\nTo edit a runtime configuration:\n\n```bash\nelyra-metadata install runtimes \\\n       --replace \\\n       --name=\"my_kubeflow_pipelines_runtime\" \\\n       --display_name=\"My Kubeflow Pipelines Runtime\" \\\n       --api_endpoint=https://kubernetes-service.ibm.com/pipeline \\\n       --api_username=username@email.com \\\n       --api_password=mynewpassword \\\n       --engine=Argo \\\n       --cos_endpoint=http://minio-service.kubeflow:9000 \\\n       --cos_username=minio \\\n       --cos_password=minio123 \\\n       --cos_bucket=test-bucket \\\n       --tags=\"['kfp', 'v1.1']\" \\\n       --schema_name=kfp\n```\n\nRefer to the [Kubeflow Pipelines Configuration settings](#kubeflow-pipelines-configuration-settings) section for an explanation of the parameters. Note that you must specify the `--name` parameter. \n\n### Deleting a runtime configuration\n\nTo delete a runtime configuration run the following command, replacing the configuration name as appropriate.\n\n```bash\nelyra-metadata remove runtimes --name=my_kubeflow_pipelines_runtime\n```\n\n## Configuration settings\n\n### Common configuration settings\n\nConfigurations include the following   common settings for all supported runtime types. The string in the headings below, which is enclosed in parentheses, denotes the CLI option name.\n\n#### Name (display_name)\n\nA user-friendly name for runtime configuration. This property is required.\n\nExample: `Kubeflow Pipelines dev environment`\n\n#### N/A (name)\n\nA unique identifier for this configuration. A value is automatically generated from `display_name`.\n\nExample: `kubeflow_pipelines_dev_environment`\n\n#### Description (description)\n\nDescription for this runtime image configuration. This property is optional.\n\nExample: `Kubeflow Pipelines deployment in QA`\n\n#### Tags (tags)\n\nZero or more tags for this runtime configuration.\n\nExample: `['test-env','airflow']`\n\n### Kubeflow Pipelines configuration settings\n\nThis section defines the settings for the Kubeflow Pipelines deployment that you want to associate with this runtime configuration.\n\n#### Kubeflow Pipelines API endpoint (api_endpoint)\n\nThe KubeFlow Pipelines API endpoint you want to utilize. This setting is required.\n\nExample: `https://kubernetes-service.ibm.com/pipeline`\n\n#### Kubeflow Pipelines user namespace (user_namespace)\nThe namespace used to run your pipeline in Kubeflow Pipelines. This setting is required if namespaces are defined in Kubeflow Pipelines. SEE NOTE.\n\nExample: `anonymous`\n\n#### Kubeflow Pipelines API endpoint username (api_username)\nUsername used to access your KubeFlow Pipelines API endpoint. This setting is required if the Kubeflow Pipelines deployment is multi-user, auth enabled.\n\nExample: `username@email.com`\n\n#### Kubeflow Pipelines API endpoint (api_password)\nPassword used to access your KubeFlow Pipelines API endpoint. This setting is required if the Kubeflow Pipelines deployment is multi-user, auth enabled.\n\nExample: `mypassword`\n\n#### Kubeflow Pipelines engine (engine)\nThe engine being used by Kubeflow Pipelines to run pipelines: `Argo` or `Tekton`. If you have access to the Kubernetes cluster where Kubeflow Pipelines is deployed, run these commands in a terminal window to determine the engine type.\n\n```\n# If this command completes successfully, the engine type is Argo.\nkubectl describe configmap -n kubeflow workflow-controller-configmap\n\n# If this command completes successfully, the engine type is Tekton.\nkubectl describe configmap -n kubeflow kfp-tekton-config\n```\n\nThe default is `Argo`.\n\nExample: `Argo`\n\n### Apache Airflow configuration settings\n\nThis section defines the settings for the Apache Airflow deployment that you want to associate with this runtime configuration.\n\n#### Apache Airflow UI endpoint (api_endpoint)\n\nThe Apache Airflow API endpoint you want to utilize. This setting is required.\n\nExample: `https://your-airflow-webserver:port`\n\n#### Apache Airflow user namespace (user_namespace)\nThe namespace used to run your DAG in Apache Airflow. The Kubernetes namespace must be configured with the correct permissions prior to use in Apache Airflow. This setting is Optional. \n\nThe default namespace is `default`.\n\nExample: `anonymous`\n\n#### GitHub API Endpoint (github_api_endpoint)\n\nThe GitHub (or GitHub Enterprise) API endpoint where the git client will attempt to connect. This setting is required. Keep the default  `https://api.github.com` for github.com\n\nExample: `https://api.private.githubenterprise.com`\n\n#### GitHub DAG Repository (github_repo)\n\nThe GitHub repository that Apache Airflow utilizes to store DAGs. This setting is required and the repository must exist.\n\nExample: `user-or-org/dag-repo-name`\n\n#### GitHub DAG Repository Branch (github_branch)\nThe name of the branch in `github_repo` where DAGs are stored. \nThis setting is required and the branch must exist.\n\nExample: `dag-branch`\n\n#### GitHub Personal Access Token (github_repo_token)\nA [GitHub personal access token](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) with write access to the GitHub DAG Repository. This setting is required. \n\nExample: `766f7c267519fee7c71d7f96bdf42e646dc65433`\n\n### Cloud Storage settings\n\nThis section defines the settings for the cloud storage that you want to associate with this runtime configuration.\n\n#### Cloud Object Storage endpoint (cos_endpoint)\nThis should be the URL address of your S3-compatible Object Storage. If running an Object Storage Service within a Kubernetes cluster (Minio), you can use the Kubernetes local DNS address. This setting is required.\n\nExample: `https://minio-service.kubeflow:9000`\n\n#### Cloud Object Storage Credentials Secret (cos_secret)\n(Optional) Kubernetes secret that's defined in the specified user namespace, containing the Cloud Object Storage username and password.\nIf specified, this secret must exist on the Kubernetes cluster hosting your pipeline runtime in order to successfully\nexecute pipelines. This setting is optional but is recommended for use in shared environments to avoid exposing a user's \nCloud Object Storage credentials. \n\nExample: `my-cos-secret`\n\nThe following is an example of how your secret on the Kubernetes cluster hosting your runtime should be defined. The variable\nnames defined under `data`, must be `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` followed by each respective value \nencoded in base64. Learn how to create, deploy, or configure [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/).\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <cos_secret>\ntype: Opaque\ndata:\n  AWS_ACCESS_KEY_ID: <BASE64_ENCODED_YOUR_AWS_ACCESS_KEY_ID>\n  AWS_SECRET_ACCESS_KEY: <BASE64_ENCODED_YOUR_AWS_SECRET_ACCESS_KEY>\n```\n\n#### Cloud Object Storage username (cos_username)\nUsername used to access the Object Storage. This setting is required.\n\nExample: `minio`\n\n#### Cloud Object Storage password (cos_password)\nPassword for cos_username. This setting is required.\n\nExample: `minio123`\n\n#### Cloud Object Storage bucket name (cos_bucket)\nName of the bucket you want Elyra to store pipeline artifacts in. This setting is required. If the bucket doesn't exist, it will be created. The specified bucket name must meet the naming conventions imposed by the Object Storage service.\n\nExample: `test-bucket`\n\n> If using IBM Cloud Object Storage, you must generate a set of [HMAC Credentials](https://cloud.ibm.com/docs/services/cloud-object-storage/hmac?topic=cloud-object-storage-uhc-hmac-credentials-main)\nand grant that key at least [Writer](https://cloud.ibm.com/docs/services/cloud-object-storage/iam?topic=cloud-object-storage-iam-bucket-permissions) level privileges.\nSpecify `access_key_id` and `secret_access_key` as `cos_username` and `cos_password`, respectively.\n\n## Verifying runtime configurations\n\nThe [Elyra examples repository contains a basic pipeline](https://github.com/elyra-ai/examples/pipelines/setup_validation) that you can use to verify your runtime configurations:\n\n1. Launch JupyterLab.\n1. Clone `https://github.com/elyra-ai/examples.git` (`Git` > `Clone A Repository`) into the current working directory.\n1. In the File Browser navigate to `examples/pipelines/setup_validation/` and follow the instructions in `README.md`. If your runtime configuration is correct and the target runtime environment configured correctly, the validation pipeline should run as is without any modifications.\n\n## Troubleshooting \n\nI am seeing this error when using Elyra with Kubeflow Pipelines that is Dex enabled: \n```bash\nHTTP response body: {\"error\":\"Validate experiment request failed.: Invalid input error: Invalid resource references for experiment. Expect one namespace type with owner relationship.\n```\n- Ensure that you have logged into the Kubeflow Dex landing page (https://kubeflow.cluster:31380....) at least once with \nyour credentials via the GUI. You should have been greeted with a dialog box and request to create a new namespace. \nWithout this step complete, Elyra will not be able to create pipelines on the Kubeflow cluster. \n\n- Ensure you've configured Kubeflow Pipelines credentials and that they are correct. When using Dex, the `api_username` is typically \nyour email address and `user_namespace` is your email shortname (e.g. `elyra` for `elyra@email.org`).\n","fileAbsolutePath":"/Users/dsobryan/Documents/ElyraOS/elyra-ai-site/src/pages/user-guide/runtime-configuration.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}