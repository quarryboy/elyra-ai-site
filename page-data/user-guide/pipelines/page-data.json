{"componentChunkName":"component---src-pages-user-guide-pipelines-mdx","path":"/user-guide/pipelines/","result":{"pageContext":{"frontmatter":{"title":"Pipelines","description":"Pipelines"},"relativePagePath":"/user-guide/pipelines.mdx","titleType":"page","MdxNode":{"id":"bb7ffa79-a70c-5c50-886a-aa907d1bfd09","children":[],"parent":"74e5f53e-d78b-52b8-8cf6-19c9c1fef93c","internal":{"content":"---\ntitle: Pipelines\ndescription: Pipelines\n---\n\nexport const Title = () => (\n  <span>\n    Pipelines\n  </span>\n);\n\n<PageDescription>\n\nA runtime image configuration identifies a container image that Elyra can utilize to run Jupyter notebooks or scripts on a container platform, such as Kubernetes.\n\n</PageDescription>\n\n## Overview\n\nA _pipeline_ comprises one or more _nodes_ that are (in many cases) connected with each other to define execution _dependencies_. A node is an instance of a configurable _[component](pipeline-components.md)_ that commonly only implements a single unit of work to make it reusable. A unit of work can represent any task, such as loading data, pre-processing data, analyzing data, training a machine learning model, deploying a model for serving, querying a service, or sending an email. \n\n![Conceptual pipeline overview](../images/ai/pipelines-nodes.png)\n\nNote though that multiple components might implement the \"same\" task. For example, one component might load data from a SQL database, whereas another component might download data from S3 storage. Conceptually both components load data, but how they load it is entirely different.\n\nElyra supports two types of components: generic components and custom components. A pipeline that utilizes only generic components is called a _generic pipeline_, whereas a pipeline that utilizes generic components and/or custom components is referred to as _runtime-specific pipeline_.\n\nPipelines are assembled using the Visual Pipeline Editor. The editor includes a palette, the canvas, and a properties panel, shown on the left, in the center, and the right, respectively.\n\n![The Visual Pipeline Editor is used to assemble pipelines](../images/ai/pipeline-editor.png)\n\nPlease review the [_Best practices for file-based pipeline nodes_ topic in the _User Guide_](best-practices-file-based-nodes.md) if your pipelines include generic components.\n\nElyra pipelines support three runtime platforms:\n- Local/JupyterLab\n- [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/) (with Argo or [Tekton](https://github.com/kubeflow/kfp-tekton/) workflow engines)\n- [Apache Airflow](https://airflow.apache.org/)\n\n### Generic pipelines\n\nA generic pipeline comprises only of nodes that are implemented using generic components.\nThis Elyra release includes three generic components that allow for execution of Jupyter notebooks, Python scripts, and R scripts. \n\n![Generic pipeline](../images/ai/generic-pipeline.png)\n\nGeneric pipelines are portable, meaning they can run locally in JupyterLab, or remotely on Kubeflow Pipelines or Apache Airflow.\n\n### Runtime-specific pipelines\n\nA runtime-specific pipeline is permanently associated with a runtime platform, such as Kubeflow Pipelines or Apache Airflow. A runtime-specific pipeline may include nodes that are implemented using generic components or custom components for that runtime.\n\n![A Kubeflow Pipelines pipeline](../images/ai/runtime-specific-pipeline.png)\n\nFor illustrative purposes the Elyra component registry includes a couple example custom components. You can add your own components as outlined in [_Managing custom components_](https://elyra.readthedocs.io/en/latest/user_guide/pipeline-components.html#managing-custom-components).\n\nNote that it is not possible to convert a generic pipeline to a runtime-specific pipeline or a runtime-specific pipeline from one type to another.\n\n### Creating pipelines using the Visual Pipeline Editor\n\nThe [tutorials](/getting_started/tutorials.md) provide comprehensive step-by-step instructions for creating and running pipelines. To create a pipeline using the editor:\n\n1. Open the JupyterLab Launcher and select the desired pipeline editor type (Generic, Kubeflow Pipelines, or Apache Airflow).\n\n   ![Pipeline editor links in launcher](../images/ai/editor-links.png)\n\n2. Expand the properties panel and define the pipeline properties. Pipeline properties include a description and default values for node properties. (Support for pipeline properties varies by release.)\n\n   ![Pipeline properties](../images/ai/pipeline-properties.png)\n\n3. Drag and drop components from the palette onto the canvas or double click on a palette entry.\n\n   ![Add components from palette](../images/ai/add-components-from-palette.gif)\n\n   You can also drag and drop Jupyter notebooks, Python scripts, or R scripts from the JupyterLab _File Browser_ onto the canvas.\n\n   ![Add generic components from file browser](../images/ai/add-components-from-file-browser.gif)\n\n4. Define the dependencies between nodes by connecting them, essentially creating an execution graph.\n\n   ![Connect nodes](../images/ai/connect-nodes.gif)\n\n5. Define the runtime properties for each node. Highlight a node, right click, and select `Open Properties`. Runtime properties configure a component and govern its execution behavior.\n\n   ![Configure node](../images/ai/configure-node.gif)\n\n   Runtime properties are component specific. For generic components (Jupyter notebook, Python script, and R script) the properties are defined as follows:\n\n   **Runtime Image**\n   - Required. The container image you want to use to run the notebook or script. \n   - Example: `TensorFlow 2.0`\n\n   **CPU, GPU, and RAM**\n   - Optional. Resources that the notebook or script requires. \n\n   **File Dependencies**\n   - Optional. A list of files to be passed from the local working environment into each respective step of the pipeline. Files should be in the same directory (or subdirectory thereof) as the file it is associated with. Specify one file, directory, or expression per line. Supported patterns are `*` and `?`. \n   - Example: `dependent-script.py`\n\n   **Environment Variables**\n   - Optional. A list of environment variables to be set inside in the container.  Specify one variable/value pair per line, separated by `=`.\n   - Example: `TOKEN=value`\n\n   **Output Files**\n   - Optional. A list of files generated by the notebook inside the image to be passed as inputs to the next step of the pipeline.  Specify one file, directory, or expression per line. Supported patterns are `*` and `?`.\n   - Example: `data/*.csv`\n\n1. Associate each node with a comment to document its purpose.\n\n   ![Add comment to node](../images/ai/add-comment-to-node.gif)\n\n2. Save the pipeline file.\n\nNote: You can rename the pipeline file in the JupyterLab _File Browser_.\n\n## Running pipelines\n\nPipelines can be run from the Visual Pipeline Editor and the `elyra-pipeline` command line interface. Before you can run a pipeline on Kubeflow Pipelines or Apache Airflow you must create a [`runtime configuration`](runtime-conf.md). A runtime configuration contains information about the target environment, such as server URL and credentials.\n\n**Running a pipeline from the Visual Pipeline Editor**\n\nTo run a pipeline from the Visual Pipeline Editor:\n1. Click `Run Pipeline` in the editor's tool bar.\n\n   ![Open pipeline run wizard](../images/ai/pipeline-editor-run.png)\n\n2. For generic pipelines select a runtime platform (local, Kubeflow Pipelines, Apache Airflow) and a runtime configuration for that platform. For runtime-specific  pipelines select a runtime configuration.\n\n   ![Configure pipeline run options](../images/ai/configure-pipeline-run-options.png)\n\n3. Elyra does not include a pipeline run monitoring interface for pipelines:\n   - For local/JupyterLab execution check the console output.\n   - For Kubeflow Pipelines open the Central Dashboard link.\n   - For Apache Airflow open the web GUI link.\n\n4. The pipeline run output artifacts are stored in the following locations:\n   - For local/JupyterLab execution all artifacts are stored in the local file system.\n   - For Kubeflow Pipelines and Apache Airflow output artifacts for generic components are stored in the runtime configuration's designated object storage bucket.   \n\n**Running a pipeline from the command line interface**\n\nThe [`elyra-pipeline` command line interface](https://elyra.readthedocs.io/en/latest/user_guide/command-line-interface.html#working-with-pipelines) provides two pipeline execution commands: `run` and `submit`.\n\nUse the `elyra-pipeline run` command to run a generic pipeline in your JupyterLab environment:\n\n```bash\n$ elyra-pipeline run elyra-pipelines/a-notebook.pipeline\n```\n\nUse the `elyra-pipeline submit` command to run a generic or runtime-specific pipeline remotely on Kubeflow Pipelines or Apache Airflow, specifying a compatible runtime configuration as parameter:\n\n```bash\n$ elyra-pipeline submit elyra-pipelines/a-kubeflow.pipeline \\\n      --runtime-config kfp-shared-tekton\n```\n\nNote: Refer to the [Managing runtime configurations using the Elyra CLI](runtime-conf.html#managing-runtime-configurations-using-the-elyra-cli) topic in the _User Guide_ for details on how to list and manage runtime configurations.\n\n## Exporting a pipeline\n\nWhen you export a pipeline Elyra only prepares it for later execution, but does not upload it to the Kubeflow Pipelines or Apache Airflow server. Export performs two tasks. \nIt packages dependencies for generic components and uploads them to cloud storage and it generates pipeline code for the target runtime. \n\nBefore you can export a pipeline on Kubeflow Pipelines or Apache Airflow you must create a [`runtime configuration`](runtime-conf.md). A runtime configuration contains information about the target environment, such as server URL and credentials.\n\nTo export a pipeline from the Visual Pipeline Editor:\n1. Click `Export Pipeline` in the editor's tool bar.\n\n   ![Open pipeline run wizard](../images/ai/pipeline-editor-export.png)\n\n2. For generic pipelines select a runtime platform (local, Kubeflow Pipelines, or Apache Airflow) and a runtime configuration for that platform. For runtime-specific pipelines select a runtime configuration.\n\n3. Select an export format.\n   \n   ![Configure pipeline export options](../images/ai/configure-pipeline-export-options.png)\n\n4 Import the exported pipeline file using the Kubeflow Central Dashboard or add it to the Git repository that Apache Airflow is monitoring.   \n ","type":"Mdx","contentDigest":"5331ac806908e9d0f7ea255ee10604f8","owner":"gatsby-plugin-mdx","counter":335},"frontmatter":{"title":"Pipelines","description":"Pipelines"},"exports":{},"rawBody":"---\ntitle: Pipelines\ndescription: Pipelines\n---\n\nexport const Title = () => (\n  <span>\n    Pipelines\n  </span>\n);\n\n<PageDescription>\n\nA runtime image configuration identifies a container image that Elyra can utilize to run Jupyter notebooks or scripts on a container platform, such as Kubernetes.\n\n</PageDescription>\n\n## Overview\n\nA _pipeline_ comprises one or more _nodes_ that are (in many cases) connected with each other to define execution _dependencies_. A node is an instance of a configurable _[component](pipeline-components.md)_ that commonly only implements a single unit of work to make it reusable. A unit of work can represent any task, such as loading data, pre-processing data, analyzing data, training a machine learning model, deploying a model for serving, querying a service, or sending an email. \n\n![Conceptual pipeline overview](../images/ai/pipelines-nodes.png)\n\nNote though that multiple components might implement the \"same\" task. For example, one component might load data from a SQL database, whereas another component might download data from S3 storage. Conceptually both components load data, but how they load it is entirely different.\n\nElyra supports two types of components: generic components and custom components. A pipeline that utilizes only generic components is called a _generic pipeline_, whereas a pipeline that utilizes generic components and/or custom components is referred to as _runtime-specific pipeline_.\n\nPipelines are assembled using the Visual Pipeline Editor. The editor includes a palette, the canvas, and a properties panel, shown on the left, in the center, and the right, respectively.\n\n![The Visual Pipeline Editor is used to assemble pipelines](../images/ai/pipeline-editor.png)\n\nPlease review the [_Best practices for file-based pipeline nodes_ topic in the _User Guide_](best-practices-file-based-nodes.md) if your pipelines include generic components.\n\nElyra pipelines support three runtime platforms:\n- Local/JupyterLab\n- [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/) (with Argo or [Tekton](https://github.com/kubeflow/kfp-tekton/) workflow engines)\n- [Apache Airflow](https://airflow.apache.org/)\n\n### Generic pipelines\n\nA generic pipeline comprises only of nodes that are implemented using generic components.\nThis Elyra release includes three generic components that allow for execution of Jupyter notebooks, Python scripts, and R scripts. \n\n![Generic pipeline](../images/ai/generic-pipeline.png)\n\nGeneric pipelines are portable, meaning they can run locally in JupyterLab, or remotely on Kubeflow Pipelines or Apache Airflow.\n\n### Runtime-specific pipelines\n\nA runtime-specific pipeline is permanently associated with a runtime platform, such as Kubeflow Pipelines or Apache Airflow. A runtime-specific pipeline may include nodes that are implemented using generic components or custom components for that runtime.\n\n![A Kubeflow Pipelines pipeline](../images/ai/runtime-specific-pipeline.png)\n\nFor illustrative purposes the Elyra component registry includes a couple example custom components. You can add your own components as outlined in [_Managing custom components_](https://elyra.readthedocs.io/en/latest/user_guide/pipeline-components.html#managing-custom-components).\n\nNote that it is not possible to convert a generic pipeline to a runtime-specific pipeline or a runtime-specific pipeline from one type to another.\n\n### Creating pipelines using the Visual Pipeline Editor\n\nThe [tutorials](/getting_started/tutorials.md) provide comprehensive step-by-step instructions for creating and running pipelines. To create a pipeline using the editor:\n\n1. Open the JupyterLab Launcher and select the desired pipeline editor type (Generic, Kubeflow Pipelines, or Apache Airflow).\n\n   ![Pipeline editor links in launcher](../images/ai/editor-links.png)\n\n2. Expand the properties panel and define the pipeline properties. Pipeline properties include a description and default values for node properties. (Support for pipeline properties varies by release.)\n\n   ![Pipeline properties](../images/ai/pipeline-properties.png)\n\n3. Drag and drop components from the palette onto the canvas or double click on a palette entry.\n\n   ![Add components from palette](../images/ai/add-components-from-palette.gif)\n\n   You can also drag and drop Jupyter notebooks, Python scripts, or R scripts from the JupyterLab _File Browser_ onto the canvas.\n\n   ![Add generic components from file browser](../images/ai/add-components-from-file-browser.gif)\n\n4. Define the dependencies between nodes by connecting them, essentially creating an execution graph.\n\n   ![Connect nodes](../images/ai/connect-nodes.gif)\n\n5. Define the runtime properties for each node. Highlight a node, right click, and select `Open Properties`. Runtime properties configure a component and govern its execution behavior.\n\n   ![Configure node](../images/ai/configure-node.gif)\n\n   Runtime properties are component specific. For generic components (Jupyter notebook, Python script, and R script) the properties are defined as follows:\n\n   **Runtime Image**\n   - Required. The container image you want to use to run the notebook or script. \n   - Example: `TensorFlow 2.0`\n\n   **CPU, GPU, and RAM**\n   - Optional. Resources that the notebook or script requires. \n\n   **File Dependencies**\n   - Optional. A list of files to be passed from the local working environment into each respective step of the pipeline. Files should be in the same directory (or subdirectory thereof) as the file it is associated with. Specify one file, directory, or expression per line. Supported patterns are `*` and `?`. \n   - Example: `dependent-script.py`\n\n   **Environment Variables**\n   - Optional. A list of environment variables to be set inside in the container.  Specify one variable/value pair per line, separated by `=`.\n   - Example: `TOKEN=value`\n\n   **Output Files**\n   - Optional. A list of files generated by the notebook inside the image to be passed as inputs to the next step of the pipeline.  Specify one file, directory, or expression per line. Supported patterns are `*` and `?`.\n   - Example: `data/*.csv`\n\n1. Associate each node with a comment to document its purpose.\n\n   ![Add comment to node](../images/ai/add-comment-to-node.gif)\n\n2. Save the pipeline file.\n\nNote: You can rename the pipeline file in the JupyterLab _File Browser_.\n\n## Running pipelines\n\nPipelines can be run from the Visual Pipeline Editor and the `elyra-pipeline` command line interface. Before you can run a pipeline on Kubeflow Pipelines or Apache Airflow you must create a [`runtime configuration`](runtime-conf.md). A runtime configuration contains information about the target environment, such as server URL and credentials.\n\n**Running a pipeline from the Visual Pipeline Editor**\n\nTo run a pipeline from the Visual Pipeline Editor:\n1. Click `Run Pipeline` in the editor's tool bar.\n\n   ![Open pipeline run wizard](../images/ai/pipeline-editor-run.png)\n\n2. For generic pipelines select a runtime platform (local, Kubeflow Pipelines, Apache Airflow) and a runtime configuration for that platform. For runtime-specific  pipelines select a runtime configuration.\n\n   ![Configure pipeline run options](../images/ai/configure-pipeline-run-options.png)\n\n3. Elyra does not include a pipeline run monitoring interface for pipelines:\n   - For local/JupyterLab execution check the console output.\n   - For Kubeflow Pipelines open the Central Dashboard link.\n   - For Apache Airflow open the web GUI link.\n\n4. The pipeline run output artifacts are stored in the following locations:\n   - For local/JupyterLab execution all artifacts are stored in the local file system.\n   - For Kubeflow Pipelines and Apache Airflow output artifacts for generic components are stored in the runtime configuration's designated object storage bucket.   \n\n**Running a pipeline from the command line interface**\n\nThe [`elyra-pipeline` command line interface](https://elyra.readthedocs.io/en/latest/user_guide/command-line-interface.html#working-with-pipelines) provides two pipeline execution commands: `run` and `submit`.\n\nUse the `elyra-pipeline run` command to run a generic pipeline in your JupyterLab environment:\n\n```bash\n$ elyra-pipeline run elyra-pipelines/a-notebook.pipeline\n```\n\nUse the `elyra-pipeline submit` command to run a generic or runtime-specific pipeline remotely on Kubeflow Pipelines or Apache Airflow, specifying a compatible runtime configuration as parameter:\n\n```bash\n$ elyra-pipeline submit elyra-pipelines/a-kubeflow.pipeline \\\n      --runtime-config kfp-shared-tekton\n```\n\nNote: Refer to the [Managing runtime configurations using the Elyra CLI](runtime-conf.html#managing-runtime-configurations-using-the-elyra-cli) topic in the _User Guide_ for details on how to list and manage runtime configurations.\n\n## Exporting a pipeline\n\nWhen you export a pipeline Elyra only prepares it for later execution, but does not upload it to the Kubeflow Pipelines or Apache Airflow server. Export performs two tasks. \nIt packages dependencies for generic components and uploads them to cloud storage and it generates pipeline code for the target runtime. \n\nBefore you can export a pipeline on Kubeflow Pipelines or Apache Airflow you must create a [`runtime configuration`](runtime-conf.md). A runtime configuration contains information about the target environment, such as server URL and credentials.\n\nTo export a pipeline from the Visual Pipeline Editor:\n1. Click `Export Pipeline` in the editor's tool bar.\n\n   ![Open pipeline run wizard](../images/ai/pipeline-editor-export.png)\n\n2. For generic pipelines select a runtime platform (local, Kubeflow Pipelines, or Apache Airflow) and a runtime configuration for that platform. For runtime-specific pipelines select a runtime configuration.\n\n3. Select an export format.\n   \n   ![Configure pipeline export options](../images/ai/configure-pipeline-export-options.png)\n\n4 Import the exported pipeline file using the Kubeflow Central Dashboard or add it to the Git repository that Apache Airflow is monitoring.   \n ","fileAbsolutePath":"/Users/dsobryan/Documents/ElyraOS/elyra-ai-site/src/pages/user-guide/pipelines.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}