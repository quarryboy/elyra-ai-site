{"componentChunkName":"component---src-pages-recipes-visualizing-output-mdx","path":"/recipes/visualizing-output/","result":{"pageContext":{"frontmatter":{"title":"Visualizing output from your notebooks or Python scripts","description":"Visualizing output from your notebooks or Python scripts"},"relativePagePath":"/recipes/visualizing-output.mdx","titleType":"page","MdxNode":{"id":"01e35e92-211d-5419-b0b2-576515818326","children":[],"parent":"1bb64c0e-71d9-5312-98cf-be58a7095331","internal":{"content":"---\ntitle: Visualizing output from your notebooks or Python scripts\ndescription: Visualizing output from your notebooks or Python scripts\n---\n\nexport const Title = () => (\n  <span>\n    Visualizing output from your notebooks or Python scripts\n  </span>\n);\n\n<PageDescription>\n\nPipelines that you run on Kubeflow Pipelines can optionally produce output that is rendered in the Kubeflow Pipelines UI. For example, a model training script might expose quality metrics.\n\n\n</PageDescription>\n\n\nYou can try the visualizations shown in this document using [this pipeline in the Elyra examples repository](https://github.com/elyra-ai/examples/tree/master/pipelines/visualize_output_in_kubeflow_pipelines_ui).\n\n## Visualizing output using the Kubeflow Pipelines output viewer\n\nThe output viewer in the Kubeflow Pipelines UI can render output such as a confusion matrix, ROC curve, or markdown, that is displayed in the Kubeflow Pipelines UI.\n\n![Example notebook output](../images/ai/kfp_mlpipeline_ui_metadata.png)\n\nTo produce this output add code to your notebook or Python script that creates a file named `mlpipeline-ui-metadata.json` in the current working directory. Refer to [_Visualize Results in the Pipelines UI_ in the Kubeflow Pipelines documentation](https://www.kubeflow.org/docs/pipelines/sdk/output-viewer/#introduction) to learn about supported visualizations and the format of the `mlpipeline-ui-metadata.json` file. \n\nThe following code snippet produces a confusion matrix that is rendered in the output viewer as shown above: \n\n```\nimport json\nimport pandas as pd \n\nmatrix = [\n    ['yummy', 'yummy', 10],\n    ['yummy', 'not yummy', 2],\n    ['not yummy', 'yummy', 6],\n    ['not yummy', 'not yummy', 7]\n]\n\ndf = pd.DataFrame(matrix,columns=['target','predicted','count'])\n\nmetadata = {\n    \"outputs\": [\n        {\n            \"type\": \"confusion_matrix\",\n            \"format\": \"csv\",\n            \"schema\": [\n                {\n                    \"name\": \"target\",\n                    \"type\": \"CATEGORY\"\n                },\n                {\n                    \"name\": \"predicted\",\n                    \"type\": \"CATEGORY\"\n                },\n                {\n                    \"name\": \"count\",\n                    \"type\": \"NUMBER\"\n                }\n            ],\n            \"source\": df.to_csv(header=False, index=False),\n            \"storage\": \"inline\",\n            \"labels\": [\n                \"yummy\",\n                \"not yummy\"\n            ]\n        }\n    ]\n}\n\nwith open('mlpipeline-ui-metadata.json', 'w') as f:\n    json.dump(metadata, f)\n```\n\n\nNote that the output is displayed only after notebook or Python script processing has completed.\n\n## Visualizing scalar performance metrics in the Kubeflow Pipelines UI\n\nIf your notebooks or Python scripts calculate scalar performance metrics they can be displayed as part of the run output in the Kubeflow Pipelines UI.\n\n![Example notebook output](../images/ai/kfp_run_metrics.png)\n\nTo expose the metrics, add code to the notebook or Python script that stores them in a file named `mlpipeline-metrics.json` in the current working directory. Refer to [_Pipeline Metrics_ in the Kubeflow Pipelines documentation](https://www.kubeflow.org/docs/pipelines/sdk/pipelines-metrics/) to learn more about the content of this file.\n\nThe following code snippet produces this file and records two metrics: \n\n```\n  import json\n   \n  # ...\n  # calculate Accuracy classification score\n  accuracy_score = 0.6\n  # calculate Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n  roc_auc_score = 0.75\n \n  metrics = {\n    'metrics': [\n        {\n            'name': 'accuracy-score',\n            'numberValue':  accuracy_score,\n            'format': 'PERCENTAGE'\n        },\n        {\n            'name': 'roc-auc-score',\n            'numberValue':  roc_auc_score,\n            'format': 'RAW'       \n        }\n    ]\n  }\n\n  with open('mlpipeline-metrics.json', 'w') as f:\n    json.dump(metrics, f)\n```\n\nNote that the metrics are displayed after notebook or Python script processing has completed.\n","type":"Mdx","contentDigest":"421a0b62afca1585f5d06af281fab954","owner":"gatsby-plugin-mdx","counter":345},"frontmatter":{"title":"Visualizing output from your notebooks or Python scripts","description":"Visualizing output from your notebooks or Python scripts"},"exports":{},"rawBody":"---\ntitle: Visualizing output from your notebooks or Python scripts\ndescription: Visualizing output from your notebooks or Python scripts\n---\n\nexport const Title = () => (\n  <span>\n    Visualizing output from your notebooks or Python scripts\n  </span>\n);\n\n<PageDescription>\n\nPipelines that you run on Kubeflow Pipelines can optionally produce output that is rendered in the Kubeflow Pipelines UI. For example, a model training script might expose quality metrics.\n\n\n</PageDescription>\n\n\nYou can try the visualizations shown in this document using [this pipeline in the Elyra examples repository](https://github.com/elyra-ai/examples/tree/master/pipelines/visualize_output_in_kubeflow_pipelines_ui).\n\n## Visualizing output using the Kubeflow Pipelines output viewer\n\nThe output viewer in the Kubeflow Pipelines UI can render output such as a confusion matrix, ROC curve, or markdown, that is displayed in the Kubeflow Pipelines UI.\n\n![Example notebook output](../images/ai/kfp_mlpipeline_ui_metadata.png)\n\nTo produce this output add code to your notebook or Python script that creates a file named `mlpipeline-ui-metadata.json` in the current working directory. Refer to [_Visualize Results in the Pipelines UI_ in the Kubeflow Pipelines documentation](https://www.kubeflow.org/docs/pipelines/sdk/output-viewer/#introduction) to learn about supported visualizations and the format of the `mlpipeline-ui-metadata.json` file. \n\nThe following code snippet produces a confusion matrix that is rendered in the output viewer as shown above: \n\n```\nimport json\nimport pandas as pd \n\nmatrix = [\n    ['yummy', 'yummy', 10],\n    ['yummy', 'not yummy', 2],\n    ['not yummy', 'yummy', 6],\n    ['not yummy', 'not yummy', 7]\n]\n\ndf = pd.DataFrame(matrix,columns=['target','predicted','count'])\n\nmetadata = {\n    \"outputs\": [\n        {\n            \"type\": \"confusion_matrix\",\n            \"format\": \"csv\",\n            \"schema\": [\n                {\n                    \"name\": \"target\",\n                    \"type\": \"CATEGORY\"\n                },\n                {\n                    \"name\": \"predicted\",\n                    \"type\": \"CATEGORY\"\n                },\n                {\n                    \"name\": \"count\",\n                    \"type\": \"NUMBER\"\n                }\n            ],\n            \"source\": df.to_csv(header=False, index=False),\n            \"storage\": \"inline\",\n            \"labels\": [\n                \"yummy\",\n                \"not yummy\"\n            ]\n        }\n    ]\n}\n\nwith open('mlpipeline-ui-metadata.json', 'w') as f:\n    json.dump(metadata, f)\n```\n\n\nNote that the output is displayed only after notebook or Python script processing has completed.\n\n## Visualizing scalar performance metrics in the Kubeflow Pipelines UI\n\nIf your notebooks or Python scripts calculate scalar performance metrics they can be displayed as part of the run output in the Kubeflow Pipelines UI.\n\n![Example notebook output](../images/ai/kfp_run_metrics.png)\n\nTo expose the metrics, add code to the notebook or Python script that stores them in a file named `mlpipeline-metrics.json` in the current working directory. Refer to [_Pipeline Metrics_ in the Kubeflow Pipelines documentation](https://www.kubeflow.org/docs/pipelines/sdk/pipelines-metrics/) to learn more about the content of this file.\n\nThe following code snippet produces this file and records two metrics: \n\n```\n  import json\n   \n  # ...\n  # calculate Accuracy classification score\n  accuracy_score = 0.6\n  # calculate Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n  roc_auc_score = 0.75\n \n  metrics = {\n    'metrics': [\n        {\n            'name': 'accuracy-score',\n            'numberValue':  accuracy_score,\n            'format': 'PERCENTAGE'\n        },\n        {\n            'name': 'roc-auc-score',\n            'numberValue':  roc_auc_score,\n            'format': 'RAW'       \n        }\n    ]\n  }\n\n  with open('mlpipeline-metrics.json', 'w') as f:\n    json.dump(metrics, f)\n```\n\nNote that the metrics are displayed after notebook or Python script processing has completed.\n","fileAbsolutePath":"/Users/dsobryan/Documents/ElyraOS/elyra-ai-site/src/pages/recipes/visualizing-output.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}